{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563ad864-d6d8-4379-9160-6ee43d62dbde",
   "metadata": {},
   "source": [
    "## 27.Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88240-8fc4-4b77-a174-84208ab8d90a",
   "metadata": {},
   "source": [
    "### 27.1本章工作任务\n",
    "\n",
    "### 27.2本章技能目标\n",
    "\n",
    "### 27.3本章简介\n",
    "\n",
    "### 27.4理论讲解部分\n",
    "**Beautiful_Soup对象中的方法实现信息定向抓取的原理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9bfaf9-75c0-4161-acf7-5645a6fac5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用Beautiful Soup 对象中的方法实现信息定向抓取\n",
    "\n",
    "#步骤1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import  html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb4eea8-80b1-414e-8bbc-2d941b3f5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤2:构建浏览器访问网址时的Headers标签信息,模拟浏览器访问网址\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}    #构造请求头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de688c8-582a-471f-b78d-456671afd9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '韩正会见美国前财政部长保尔森', 'time': '2019年04月11日19:21 来源：新华社', 'content': ['新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光阁会见美国前财政部长保尔森。', '韩正表示，中美建交40年来，两国关系经历各种考验，取得了历史性发展。中美在维护世界和平稳定、促进全球发展繁荣方面拥有广泛利益、肩负着重要责任。要在两国元首重要共识的指引下，推动中美经贸合作和两国关系健康稳定向前发展。加强生态环境保护是中国实现高质量发展的必然要求，中方愿意开展各种形式的国际合作，愿意与保尔森基金会共同努力，在生态环境保护方面进一步拓宽合作领域。', '保尔森表示，美中关系是世界上最重要的双边关系之一。保尔森基金会愿意积极推动美中贸易投资、环境保护、清洁能源、绿色金融等领域的合作。']}\n",
      "{'title': '习近平与独龙族的故事', 'time': '2019年04月11日19:20 来源：新华网', 'content': ['新华网', '金佳绪', '【学习进行时】4月10日，习近平给云南省贡山县独龙江乡群众回信。总书记在信中写道，“得知这个消息，我很高兴”。回信背后有哪些故事？新华社《学习进行时》原创品牌栏目“讲习所”挖掘梳理，和您一同探寻。', '4月10日，习近平给云南省贡山县独龙江乡群众回信，祝贺独龙族实现整族脱贫。', '总书记说，得知这个消息，我很高兴，向你们表示衷心的祝贺！', '给独龙乡群众回信，总书记说了啥', '2018年，独龙江乡6个行政村整体脱贫，独龙族实现整族脱贫，当地群众委托乡党委给习近平总书记写信，汇报独龙族实现整族脱贫的喜讯。', '2019年4月10日，习近平给乡亲们回信，祝贺独龙族实现整族脱贫。', '在信里，习近平说：', '“让各族群众都过上好日子，是我一直以来的心愿，也是我们共同奋斗的目标。新中国成立后，独龙族告别了刀耕火种的原始生活。进入新时代，独龙族摆脱了长期存在的贫困状况。这生动说明，有党的坚强领导，有广大人民群众的团结奋斗，人民追求幸福生活的梦想一定能够实现。”', '“脱贫只是第一步，更好的日子还在后头。”习近平勉励乡亲们再接再厉、奋发图强，同心协力建设好家乡、守护好边疆，努力创造独龙族更加美好的明天。', '从“整体贫困”到“整族脱贫”的沧桑巨变', '独龙族是我国28个人口较少民族之一，也是新中国成立初期一个从原始社会末期直接过渡到社会主义社会的少数民族，主要聚居在云南省贡山县独龙江乡。当地地处深山峡谷，自然条件恶劣，一直是云南乃至全国最为贫穷的地区之一。', '摆脱贫困，过上美好生活，这是独龙族同胞一直以来的期盼。如今，这个愿望变成了现实。', '2018年年底，作为人口较少的“直过民族”，独龙族从整体贫困实现了整族脱贫，贫困发生率下降到了2.63%，独龙江乡1086户群众全部住进了新房，所有自然村都通了硬化路，4G网络、广播电视信号覆盖到全乡，种草果、采蜂蜜、养独龙牛，乡亲们的收入增加了，孩子们享受着14年免费教育，群众看病有了保障……', '习近平与独龙族的“情缘”', '很多人对这个生活在偏远地区人数较少的少数民族有些陌生，知之甚少。但习近平却表示，“我们并不陌生，因为有书信往来。”', '2014年元旦前夕，贡山县干部群众致信习近平总书记，汇报了当地经济社会发展和人民生活改善的情况，报告了多年期盼的高黎贡山独龙江公路隧道即将贯通的消息，习近平接到信后立即给他们回信：“向独龙族的乡亲们表示祝贺！”希望独龙族群众“加快脱贫致富步伐，早日实现与全国其他兄弟民族一道过上小康生活的美好梦想”。', '总书记对独龙族同胞的牵挂，远不止书信。', '2015年1月，习近平在云南考察。他仍关注着高黎贡山隧道建设，关注着独龙族干部群众生活发生的变化。带着对贡山县干部群众尤其是独龙族乡亲们的惦念，习近平在这次紧张的行程中特地抽出时间，把当初写信的5位干部群众和2位独龙族妇女，专程接到昆明来见面。', '“建一套新房多少钱？”“原来出山要多长时间？”……此次见面，习近平对乡亲们的生活情况细问详察，共同分享沧桑巨变带来的喜悦，对干部群众寄语频频。', '“我今天特别高兴，能够在这里同贡山独龙族怒族自治县的代表们见面。”习近平说，独龙族这个名字是周总理起的，虽然只有6900多人，人口不多，也是中华民族大家庭平等的一员，在中华人民共和国、中华民族大家庭之中骄傲地、有尊严地生活着，在中国共产党领导下，同各民族人民一起努力工作，为全面建成小康社会的目标奋斗。', '总书记指出，独龙族和其他一些少数民族的沧桑巨变，证明了中国特色社会主义制度的优越性。前面的任务还很艰巨，我们要继续发挥我国制度的优越性，继续把工作做好、事情办好。', '“全面建成小康社会，一个民族都不能少”，是全国人民的心愿，更是以习近平同志为核心的党中央的坚定决心。当又一个少数民族整体脱贫的好消息传来，总书记怎能不由衷高兴！', '点击进入专题']}\n"
     ]
    }
   ],
   "source": [
    "#步骤3 获取指定URL的页面内容并根据抓取模板进行待抓取信息的提取\n",
    "\n",
    "def get_detail(url):\n",
    "    new = {} #定义字典,用于存储解析\n",
    "    response = requests.get(url, headers = HEADERS)    #用requests包提交网络请求,获取HTML页面内容\n",
    "    text = response.content.decode(\"gb18030\")\n",
    "    soup = BeautifulSoup(text, 'html5lib')    #创建BeautifulSoup对象，使用html5lib解析器对text进行解析\n",
    "    title = soup.find('div', class_='clearfix w1000_320 text_title')    #运用find方法，找到第一个class\n",
    "    new['title'] = title.h1.get_text()    #将解析,h1为标题,get_text()用于从html的字符串中提取文本\n",
    "    box01 = soup.find('div', class_='box01')\n",
    "    whenAndWhere = box01.select('div')[0]  \n",
    "    new['time'] = \" \".join(whenAndWhere.get_text().split())\n",
    "    box_con = soup.find(\"div\", class_='box_con')\n",
    "    contents = box_con.select(\"p\")\n",
    "    content = []\n",
    "    for c in contents:\n",
    "        content.extend(c.get_text().split())\n",
    "    new['content'] = content\n",
    "    return new\n",
    "\n",
    "def spider(base_url):\n",
    "    news = []\n",
    "    for url in base_url:\n",
    "        new = get_detail(url)\n",
    "        news.append(new)\n",
    "    return news\n",
    "\n",
    "base_url = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html' ,\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html' ]#指定待抓取的新闻详情页url\n",
    "\n",
    "news = spider(base_url)\n",
    "for i in range(0,len(news)):\n",
    "    print(news[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a61ccf8-cb5a-4dda-a081-d5e57f19e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤6\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecf6536-04cc-4087-9b9d-1e6b248a0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(news, columns=['time', 'title', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c65d4b4-e3ed-42e2-a5c2-a6faf15759bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('news_result.csv')\n",
    "df.to_excel('news_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2b8606-ae4d-4652-9b75-0a1f9581fba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019年04月11日19:21 来源：新华社</td>\n",
       "      <td>韩正会见美国前财政部长保尔森</td>\n",
       "      <td>['新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019年04月11日19:20 来源：新华网</td>\n",
       "      <td>习近平与独龙族的故事</td>\n",
       "      <td>['新华网', '金佳绪', '【学习进行时】4月10日，习近平给云南省贡山县独龙江乡群众回...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     time           title  \\\n",
       "0           0  2019年04月11日19:21 来源：新华社  韩正会见美国前财政部长保尔森   \n",
       "1           1  2019年04月11日19:20 来源：新华网      习近平与独龙族的故事   \n",
       "\n",
       "                                             content  \n",
       "0  ['新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光...  \n",
       "1  ['新华网', '金佳绪', '【学习进行时】4月10日，习近平给云南省贡山县独龙江乡群众回...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('news_result.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc73f5f-07b6-4990-b026-4987acec4e94",
   "metadata": {},
   "source": [
    "### 27.6本章作业\n",
    "- 实现本章案例，即抓取人民日报网新闻信息，并将信息内容可视化。\n",
    "- 设计抓取程序，采用Beautiful Soup对象，抓取某大学教务处的通知信息，将上述通知信息(标题、时间、内容)可视化，并存入Excel文档中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908f976-4e93-4615-b55a-74f64ed65566",
   "metadata": {},
   "source": [
    "关键点\n",
    "提供的代码正确使用了 Beautiful Soup 来从指定 HTML 结构中提取标题、时间和内容。\n",
    "代码通过发送 GET 请求并使用自定义 User-Agent 头模拟浏览器，解码响应内容并使用 html5lib 解析器解析 HTML。\n",
    "提取信息时，代码通过类名找到特定的 div 元素，获取文本内容。\n",
    "建议改进：时间字段可能包含来源信息，可分离；内容字段目前为单词列表，可优化为段落字符串。\n",
    "代码功能概述\n",
    "代码实现了一个网络爬虫，使用 Beautiful Soup 从人民日报网站（People's Daily Website）的新闻页面提取信息。主函数 get_detail 处理单个 URL，提取标题、时间和内容；spider 函数处理多个 URL，并将结果保存为 CSV 和 Excel 文件。\n",
    "\n",
    "Beautiful Soup 的使用\n",
    "代码利用 Beautiful Soup 的 find 和 select 方法定位 HTML 元素。例如：\n",
    "\n",
    "使用 soup.find('div', class_='clearfix w1000_320 text_title') 找到标题所在的 div，然后通过 .h1.get_text() 提取标题文本。\n",
    "使用 soup.find('div', class_='box01') 和 .select('div')[0] 提取时间和来源信息。\n",
    "使用 soup.find(\"div\", class_='box_con').select(\"p\") 获取内容段落，并通过 get_text() 提取文本。\n",
    "潜在改进\n",
    "时间和来源分离：当前时间字段包含日期/时间和来源（如“2019-04-11 19:21 sources：新华网”），建议通过分割字符串（如按“sources：”分割）分别存储。\n",
    "内容格式优化：当前内容字段为单词列表，可能不适合后续处理，建议改为段落字符串列表或用换行符连接的单字符串。\n",
    "调查报告：Beautiful Soup 信息定向抓取的详细分析\n",
    "本文详细分析了提供的 Python 代码，该代码使用 Beautiful Soup 实现从指定网页的定向信息提取，特别针对人民日报网站的新闻页面。以下是全面的分析，包括代码功能、技术细节和潜在改进建议。\n",
    "\n",
    "代码结构与功能\n",
    "代码包含两个主要函数：\n",
    "\n",
    "get_detail(url)：处理单个 URL，执行以下步骤：\n",
    "使用 requests.get 发送 GET 请求，设置 User-Agent 头模拟浏览器访问。\n",
    "解码响应内容，使用 “gb18030” 编码（适合处理 GB2312 页面）。\n",
    "使用 Beautiful Soup 和 html5lib 解析器解析 HTML。\n",
    "提取信息：\n",
    "标题：通过 find('div', class_='clearfix w1000_320 text_title') 找到标题 div，然后 .h1.get_text() 获取文本。\n",
    "时间：通过 find('div', class_='box01') 和 .select('div')[0].get_text() 获取时间和来源信息，处理后存储。\n",
    "内容：通过 find(\"div\", class_='box_con').select(\"p\") 获取所有段落，提取文本并分割为单词列表。\n",
    "返回包含标题、时间和内容的字典。\n",
    "spider(base_url)：处理多个 URL，调用 get_detail 并收集结果，最后返回新闻列表。\n",
    "代码还包括数据保存部分，使用 pandas 将结果保存为 CSV 和 Excel 文件，列名包括 'time'、'title' 和 'content'。\n",
    "\n",
    "Beautiful Soup 方法的应用\n",
    "Beautiful Soup 提供了多种方法用于 HTML 解析和信息提取，代码中主要使用了以下方法：\n",
    "\n",
    "find：查找第一个匹配的元素，例如 soup.find('div', class_='clearfix w1000_320 text_title')。\n",
    "select：使用 CSS 选择器查找元素，例如 box01.select('div')[0]。\n",
    "get_text：从 HTML 元素中提取纯文本，忽略标签，例如 title.h1.get_text()。\n",
    "这些方法结合使用，实现了从复杂 HTML 结构中精准定位和提取目标信息。\n",
    "\n",
    "HTML 结构分析\n",
    "根据提供的 HTML 示例（页面编码为 GB2312，标题为“韩正会见美国前财政部长保尔森”），代码的提取逻辑与页面结构匹配：\n",
    "\n",
    "标题位于 class=\"clearfix w1000_320 text_title\" 的 div 内，h1 标签包含标题文本。\n",
    "时间和来源位于 class=\"box01\" 的 div 内，第一个 div 包含“2019-04-11 19:21 sources：新华网”等信息。\n",
    "内容位于 class=\"box_con\" 的 div 内，所有 p 标签包含新闻正文。\n",
    "编码与解析\n",
    "代码使用 “gb18030” 解码响应内容，这是正确的选择，因为 gb18030 兼容 GB2312，适合处理中文网页。使用 html5lib 解析器确保对复杂 HTML 的鲁棒解析，但也可考虑使用 lxml 解析器以提高性能。\n",
    "\n",
    "潜在问题与改进建议\n",
    "通过分析，识别以下潜在问题及改进方向：\n",
    "\n",
    "时间字段包含来源：\n",
    "当前 new['time'] 存储了“2019-04-11 19:21 sources：新华网”，可能不满足用户需求。\n",
    "建议改进：通过字符串分割（如按“sources：”分割），分别存储时间和来源。例如：\n",
    "python\n",
    "\n",
    "Collapse\n",
    "\n",
    "Wrap\n",
    "\n",
    "Copy\n",
    "whenAndWhere = box01.select('div')[0].get_text()\n",
    "time_part = whenAndWhere.split(\" sources：\")[0].strip()\n",
    "new['time'] = time_part\n",
    "new['source'] = whenAndWhere.split(\" sources：\")[1].strip()\n",
    "这样可以更清晰地分离信息，方便后续处理。\n",
    "内容格式不优：\n",
    "当前 new['content'] 是所有段落文本分割后的单词列表（如 [“新华社”, “北京”, “4月11日”]），在 CSV/Excel 中可能不易阅读。\n",
    "建议改进：\n",
    "选项一：存储为段落列表，例如 new['content'] = [p.get_text() for p in contents]，每个元素为一段完整文本。\n",
    "选项二：存储为单字符串，使用换行符连接，例如 new['content'] = '\\n'.join([p.get_text() for p in contents])。\n",
    "这些格式更适合保存和展示新闻内容。\n",
    "网站结构变化风险：\n",
    "代码依赖特定类名（如 clearfix w1000_320 text_title），如果网站结构更新，可能会导致提取失败。\n",
    "建议：定期检查目标网站的 HTML 结构，或使用更通用的选择器（如 XPath）以提高鲁棒性。\n",
    "数据保存与应用\n",
    "代码使用 pandas 将结果保存为 CSV 和 Excel 文件，列名包括 'time'、'title' 和 'content'。考虑到内容字段的格式问题，建议在保存前优化数据结构，确保文件易于阅读和分析。\n",
    "\n",
    "总结\n",
    "提供的代码正确实现了 Beautiful Soup 的信息定向抓取功能，适合从人民日报新闻页面提取标题、时间和内容。然而，通过分析发现时间和来源的混合存储以及内容格式的潜在问题，建议进行上述改进以提升实用性。总体而言，该代码为网络爬虫初学者提供了良好的示例，展示了 Beautiful Soup 在网页信息提取中的强大能力。\n",
    "\n",
    "Key Citations\n",
    "People's Daily Website Detailed News Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e550d43-94a4-4acd-8416-15ef685bcae3",
   "metadata": {},
   "source": [
    "- 源代码代码优化后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb66781c-c758-4493-a3cd-45f46ac05681",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m news\n\u001b[0;32m     49\u001b[0m base_url \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025481.html\u001b[39m\u001b[38;5;124m'\u001b[39m ,\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025480.html\u001b[39m\u001b[38;5;124m'\u001b[39m ]\u001b[38;5;66;03m#指定待抓取的新闻详情页url\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m news \u001b[38;5;241m=\u001b[39m \u001b[43mspider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(news)):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(news[i])\n",
      "Cell \u001b[1;32mIn[14], line 45\u001b[0m, in \u001b[0;36mspider\u001b[1;34m(base_url)\u001b[0m\n\u001b[0;32m     43\u001b[0m news \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m base_url:\n\u001b[1;32m---> 45\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43mget_detail\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     news\u001b[38;5;241m.\u001b[39mappend(new)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m news\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36mget_detail\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     30\u001b[0m time_part \u001b[38;5;241m=\u001b[39m whenAndWhere\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sources：\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     31\u001b[0m new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m time_part\n\u001b[1;32m---> 32\u001b[0m new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mwhenAndWhere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m sources：\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     34\u001b[0m box_con \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_con\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m contents \u001b[38;5;241m=\u001b[39m box_con\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#利用Beautiful Soup 对象中的方法实现信息定向抓取\n",
    "\n",
    "#步骤1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import  html5lib\n",
    "\n",
    "#步骤2:构建浏览器访问网址时的Headers标签信息,模拟浏览器访问网址\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}    #构造请求头\n",
    "\n",
    "#步骤3 获取指定URL的页面内容并根据抓取模板进行待抓取信息的提取\n",
    "\n",
    "def get_detail(url):\n",
    "    new = {} #定义字典,用于存储解析\n",
    "    response = requests.get(url, headers = HEADERS)    #用requests包提交网络请求,获取HTML页面内容\n",
    "    text = response.content.decode(\"gb18030\")\n",
    "    soup = BeautifulSoup(text, 'html5lib')    #创建BeautifulSoup对象，使用html5lib解析器对text进行解析\n",
    "    title = soup.find('div', class_='clearfix w1000_320 text_title')    #运用find方法，找到第一个class\n",
    "    new['title'] = title.h1.get_text()    #将解析,h1为标题,get_text()用于从html的字符串中提取文本\n",
    "    box01 = soup.find('div', class_='box01')\n",
    "\n",
    "    \n",
    "    #whenAndWhere = box01.select('div')[0]  \n",
    "    #new['time'] = \" \".join(whenAndWhere.get_text().split())\n",
    "    whenAndWhere = box01.select('div')[0].get_text()\n",
    "    time_part = whenAndWhere.split(\" sources：\")[0].strip()\n",
    "    new['time'] = time_part\n",
    "    new['source'] = whenAndWhere.split(\" sources：\")[1].strip()\n",
    "    \n",
    "    box_con = soup.find(\"div\", class_='box_con')\n",
    "    contents = box_con.select(\"p\")\n",
    "    content = []\n",
    "    for c in contents:\n",
    "        content.extend(c.get_text().split())\n",
    "    new['content'] = content\n",
    "    return new\n",
    "\n",
    "def spider(base_url):\n",
    "    news = []\n",
    "    for url in base_url:\n",
    "        new = get_detail(url)\n",
    "        news.append(new)\n",
    "    return news\n",
    "\n",
    "base_url = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html' ,\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html' ]#指定待抓取的新闻详情页url\n",
    "\n",
    "news = spider(base_url)\n",
    "for i in range(0,len(news)):\n",
    "    print(news[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d759e-0927-4746-8c09-bf52a3b2c649",
   "metadata": {},
   "source": [
    "- 错误分析：提供的代码正确实现了 Beautiful Soup 的信息定向抓取功能，但因时间和来源提取不鲁棒导致 IndexError。建议使用正则表达式提取时间和来源，添加错误处理，并优化内容存储格式。这些改进能提升代码的稳定性和实用性，适合处理类似新闻页面的抓取任务。\n",
    "\n",
    "- Beautiful Soup 方法的应用\n",
    "代码利用 Beautiful Soup 的 find 和 select 方法定位 HTML 元素，例如：\n",
    "\n",
    "使用 soup.find('div', class_='clearfix w1000_320 text_title') 找到标题。\n",
    "使用 soup.find('div', class_='box01') 和 .select('div')[0] 提取时间和来源。\n",
    "使用 soup.find(\"div\", class_='box_con').select(\"p\") 获取内容。\n",
    "这些方法结合使用，实现了从复杂 HTML 结构中精准定位和提取目标信息。\n",
    "\n",
    "改进建议\n",
    "通过分析，提出以下改进：\n",
    "\n",
    "时间和来源提取的鲁棒性：\n",
    "原代码依赖固定字符串分割，容易因空格数量变化失败。建议使用正则表达式：\n",
    "时间格式：(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\n",
    "来源格式：sources：(.+)\n",
    "示例代码：\n",
    "python\n",
    "\n",
    "Collapse\n",
    "\n",
    "Wrap\n",
    "\n",
    "Copy\n",
    "import re\n",
    "whenAndWhere_text = whenAndWhere.get_text()\n",
    "time_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})'\n",
    "source_pattern = r'sources：(.+)'\n",
    "match_time = re.search(time_pattern, whenAndWhere_text)\n",
    "match_source = re.search(source_pattern, whenAndWhere_text)\n",
    "if match_time:\n",
    "    new['time'] = match_time.group(1).strip()\n",
    "else:\n",
    "    new['time'] = whenAndWhere_text\n",
    "if match_source:\n",
    "    new['source'] = match_source.group(1).strip()\n",
    "else:\n",
    "    new['source'] = \"\"\n",
    "这种方法能处理不同空格数量和格式变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbdeb5d-7953-46e4-a2bf-67b0abc4b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '韩正会见美国前财政部长保尔森', 'time': '2019年04月11日19:21 来源：新华社', 'content': ['新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光阁会见美国前财政部长保尔森。', '韩正表示，中美建交40年来，两国关系经历各种考验，取得了历史性发展。中美在维护世界和平稳定、促进全球发展繁荣方面拥有广泛利益、肩负着重要责任。要在两国元首重要共识的指引下，推动中美经贸合作和两国关系健康稳定向前发展。加强生态环境保护是中国实现高质量发展的必然要求，中方愿意开展各种形式的国际合作，愿意与保尔森基金会共同努力，在生态环境保护方面进一步拓宽合作领域。', '保尔森表示，美中关系是世界上最重要的双边关系之一。保尔森基金会愿意积极推动美中贸易投资、环境保护、清洁能源、绿色金融等领域的合作。']}\n",
      "{'title': '习近平与独龙族的故事', 'time': '2019年04月11日19:20 来源：新华网', 'content': ['新华网', '金佳绪', '【学习进行时】4月10日，习近平给云南省贡山县独龙江乡群众回信。总书记在信中写道，“得知这个消息，我很高兴”。回信背后有哪些故事？新华社《学习进行时》原创品牌栏目“讲习所”挖掘梳理，和您一同探寻。', '4月10日，习近平给云南省贡山县独龙江乡群众回信，祝贺独龙族实现整族脱贫。', '总书记说，得知这个消息，我很高兴，向你们表示衷心的祝贺！', '给独龙乡群众回信，总书记说了啥', '2018年，独龙江乡6个行政村整体脱贫，独龙族实现整族脱贫，当地群众委托乡党委给习近平总书记写信，汇报独龙族实现整族脱贫的喜讯。', '2019年4月10日，习近平给乡亲们回信，祝贺独龙族实现整族脱贫。', '在信里，习近平说：', '“让各族群众都过上好日子，是我一直以来的心愿，也是我们共同奋斗的目标。新中国成立后，独龙族告别了刀耕火种的原始生活。进入新时代，独龙族摆脱了长期存在的贫困状况。这生动说明，有党的坚强领导，有广大人民群众的团结奋斗，人民追求幸福生活的梦想一定能够实现。”', '“脱贫只是第一步，更好的日子还在后头。”习近平勉励乡亲们再接再厉、奋发图强，同心协力建设好家乡、守护好边疆，努力创造独龙族更加美好的明天。', '从“整体贫困”到“整族脱贫”的沧桑巨变', '独龙族是我国28个人口较少民族之一，也是新中国成立初期一个从原始社会末期直接过渡到社会主义社会的少数民族，主要聚居在云南省贡山县独龙江乡。当地地处深山峡谷，自然条件恶劣，一直是云南乃至全国最为贫穷的地区之一。', '摆脱贫困，过上美好生活，这是独龙族同胞一直以来的期盼。如今，这个愿望变成了现实。', '2018年年底，作为人口较少的“直过民族”，独龙族从整体贫困实现了整族脱贫，贫困发生率下降到了2.63%，独龙江乡1086户群众全部住进了新房，所有自然村都通了硬化路，4G网络、广播电视信号覆盖到全乡，种草果、采蜂蜜、养独龙牛，乡亲们的收入增加了，孩子们享受着14年免费教育，群众看病有了保障……', '习近平与独龙族的“情缘”', '很多人对这个生活在偏远地区人数较少的少数民族有些陌生，知之甚少。但习近平却表示，“我们并不陌生，因为有书信往来。”', '2014年元旦前夕，贡山县干部群众致信习近平总书记，汇报了当地经济社会发展和人民生活改善的情况，报告了多年期盼的高黎贡山独龙江公路隧道即将贯通的消息，习近平接到信后立即给他们回信：“向独龙族的乡亲们表示祝贺！”希望独龙族群众“加快脱贫致富步伐，早日实现与全国其他兄弟民族一道过上小康生活的美好梦想”。', '总书记对独龙族同胞的牵挂，远不止书信。', '2015年1月，习近平在云南考察。他仍关注着高黎贡山隧道建设，关注着独龙族干部群众生活发生的变化。带着对贡山县干部群众尤其是独龙族乡亲们的惦念，习近平在这次紧张的行程中特地抽出时间，把当初写信的5位干部群众和2位独龙族妇女，专程接到昆明来见面。', '“建一套新房多少钱？”“原来出山要多长时间？”……此次见面，习近平对乡亲们的生活情况细问详察，共同分享沧桑巨变带来的喜悦，对干部群众寄语频频。', '“我今天特别高兴，能够在这里同贡山独龙族怒族自治县的代表们见面。”习近平说，独龙族这个名字是周总理起的，虽然只有6900多人，人口不多，也是中华民族大家庭平等的一员，在中华人民共和国、中华民族大家庭之中骄傲地、有尊严地生活着，在中国共产党领导下，同各民族人民一起努力工作，为全面建成小康社会的目标奋斗。', '总书记指出，独龙族和其他一些少数民族的沧桑巨变，证明了中国特色社会主义制度的优越性。前面的任务还很艰巨，我们要继续发挥我国制度的优越性，继续把工作做好、事情办好。', '“全面建成小康社会，一个民族都不能少”，是全国人民的心愿，更是以习近平同志为核心的党中央的坚定决心。当又一个少数民族整体脱贫的好消息传来，总书记怎能不由衷高兴！', '点击进入专题']}\n"
     ]
    }
   ],
   "source": [
    "#利用Beautiful Soup 对象中的方法实现信息定向抓取\n",
    "\n",
    "#步骤1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import  html5lib\n",
    "\n",
    "#步骤2:构建浏览器访问网址时的Headers标签信息,模拟浏览器访问网址\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}    #构造请求头\n",
    "\n",
    "#步骤3 获取指定URL的页面内容并根据抓取模板进行待抓取信息的提取\n",
    "\n",
    "def get_detail(url):\n",
    "    new = {} #定义字典,用于存储解析\n",
    "    response = requests.get(url, headers = HEADERS)    #用requests包提交网络请求,获取HTML页面内容\n",
    "    text = response.content.decode(\"gb18030\")\n",
    "    soup = BeautifulSoup(text, 'html5lib')    #创建BeautifulSoup对象，使用html5lib解析器对text进行解析\n",
    "    title = soup.find('div', class_='clearfix w1000_320 text_title')    #运用find方法，找到第一个class\n",
    "    #增加错误处理\n",
    "    if title and title.h1:\n",
    "        new['title'] = title.h1.get_text()\n",
    "    else:\n",
    "        new['title'] = \"\"\n",
    "    \n",
    "    #new['title'] = title.h1.get_text()    #将解析,h1为标题,get_text()用于从html的字符串中提取文本\n",
    "    box01 = soup.find('div', class_='box01')\n",
    "\n",
    "    \n",
    "    whenAndWhere = box01.select('div')[0]  \n",
    "    new['time'] = \" \".join(whenAndWhere.get_text().split())\n",
    "    # whenAndWhere = box01.select('div')[0].get_text()\n",
    "    # time_part = whenAndWhere.split(\" sources：\")[0].strip()\n",
    "    # new['time'] = time_part\n",
    "    # new['source'] = whenAndWhere.split(\" sources：\")[1].strip()\n",
    "    \n",
    "    box_con = soup.find(\"div\", class_='box_con')\n",
    "    contents = box_con.select(\"p\")\n",
    "    content = []\n",
    "    for c in contents:\n",
    "        content.extend(c.get_text().split())\n",
    "    new['content'] = content\n",
    "    return new\n",
    "\n",
    "def spider(base_url):\n",
    "    news = []\n",
    "    for url in base_url:\n",
    "        new = get_detail(url)\n",
    "        news.append(new)\n",
    "    return news\n",
    "\n",
    "base_url = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html' ,\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html' ]#指定待抓取的新闻详情页url\n",
    "\n",
    "news = spider(base_url)\n",
    "for i in range(0,len(news)):\n",
    "    print(news[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8b9419-39de-4d35-9145-0d710a450d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤6\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(news, columns=['time', 'title', 'content'])\n",
    "\n",
    "df.to_csv('news_homework1.csv')\n",
    "df.to_excel('news_homework1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147acdf-a3f0-4d92-8488-b0932ac10307",
   "metadata": {},
   "source": [
    "### 这是我的个人作业- 设计抓取程序，采用Beautiful Soup对象，抓取某大学教务处的通知信息，将上述通知信息(标题、时间、内容)可视化，并存入Excel文档中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d24ed-39fe-4dab-972c-a40b3aeda2be",
   "metadata": {},
   "source": [
    "- 页面结构：view-source:https://portal.chd.edu.cn/psfw/sys/tzggapp/*default/index.do#/ggll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94430747-001c-412d-87e3-2bd1ef234f96",
   "metadata": {},
   "source": [
    "**经过一番分析：学校的数据使用JavaScript编写的，是动态的，无法用单一的Beautiful Soup（静态Html），\n",
    "程序设计需要使用 Beautiful Soup 抓取大学教务处通知信息，包括标题、时间和内容，并存入 Excel 文件并可视化。\n",
    "网站可能是动态加载的，可能需要 Selenium 配合 Beautiful Soup 处理 JavaScript 渲染。\n",
    "建议先检查通知页面结构，提取信息后用 pandas 保存为 Excel，并通过打印格式化数据实现可视化。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f818d984-fb95-445c-86bb-abe43e7d2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间: 2024-12-31\n",
      "标题: 时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件-中国共产党新闻网\n",
      "内容: 2024年12月31日晚，国家主席习近平发表二〇二五年新年贺词。习近平主席回顾了2024年中国取得的不平凡成就，激励全国人民“梦虽遥，追则能达；愿虽艰，持则可圆。中国式现代化的新征程上，每一个人都是主,时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 假设这是你的 HTML 代码\n",
    "html = '''\n",
    "<html>\n",
    "<head>\n",
    "<meta http-equiv=\"content-type\" content=\"text/html;charset=UTF-8\"/>\n",
    "<meta http-equiv=\"Content-Language\" content=\"utf-8\" />\n",
    "<meta content=\"all\" name=\"robots\" />\n",
    "<title>时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件-中国共产党新闻网</title>\n",
    "<meta name=\"renderer\" content=\"webkit\">\n",
    "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n",
    "<meta name=\"keywords\" content=\",时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件\" />\n",
    "<meta name=\"description\" content=\"2024年12月31日晚，国家主席习近平发表二〇二五年新年贺词。习近平主席回顾了2024年中国取得的不平凡成就，激励全国人民“梦虽遥，追则能达；愿虽艰，持则可圆。中国式现代化的新征程上，每一个人都是主,时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件\" />\n",
    "<meta name=\"filetype\" content=\"0\" />\n",
    "<meta name=\"publishedtype\" content=\"1\" />\n",
    "<meta name=\"pagetype\" content=\"1\" />\n",
    "<meta name=\"catalogs\" content=\"164113\" />\n",
    "<meta name=\"contentid\" content=\"40393373\" />\n",
    "<meta name=\"publishdate\" content=\"2024-12-31\" />\n",
    "<meta name=\"author\" content=\"105058\" />\n",
    "<meta name=\"source\" content=\"来源：人民网-中国共产党新闻网 原创稿\" />\n",
    "<meta name=\"editor\" content=\"105058;1587\">\n",
    "<meta name=\"sourcetype\" content=\"101\">\n",
    "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0\">\n",
    "<script src=\"http://www.people.com.cn/img/MAIN/2013/08/113555/js_library/jquery-1.7.1.min.js\" type=\"text/javascript\"></script>\n",
    "<script src=\"http://www.people.com.cn/img/2013people/jquery.rmw.global.js\" language=\"javascript\" type=\"text/javascript\" charset=\"UTF-8\"></script>\n",
    "<script src=\"http://www.people.com.cn/img/2013people/www.news.js\" language=\"javascript\" type=\"text/javascript\" charset=\"UTF-8\"></script>\n",
    "<script src=\"https://cdn2-app.people.cn/static/js/synchronize.js\" type=\"text/javascript\"></script>\n",
    "<script src=\"https://cdn2-app.people.cn/static/js/moment.js\" type=\"text/javascript\"></script>\n",
    "<link rel=\"stylesheet\" href=\"http://www.people.com.cn/img/MAIN/2018/04/118320/css/swiper.min.css\">\n",
    "<link rel=\"stylesheet\" href=\"http://www.people.com.cn/img/MAIN/2020/12/120684/css/share2020.css\" charset=\"utf-8\">\n",
    "<link rel=\"stylesheet\" href=\"http://www.people.com.cn/img/MAIN/2020/12/120684/css/banner.css\">\n",
    "<link href=\"http://www.people.com.cn/img/MAIN/2020/12/120684/css/pageg.css\" type=\"text/css\" rel=\"stylesheet\" media=\"all\" />\n",
    "<script src=\"http://www.people.com.cn/img/MAIN/2018/04/118320/js/swiper.min.js\"></script>\n",
    "<script type=\"text/javascript\">\n",
    "'''\n",
    "\n",
    "# 解析 HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 提取时间\n",
    "time = soup.find('meta', {'name': 'publishdate'})['content']\n",
    "\n",
    "# 提取标题\n",
    "title = soup.find('title').text\n",
    "\n",
    "# 提取内容\n",
    "content = soup.find('meta', {'name': 'description'})['content']\n",
    "\n",
    "# 输出结果\n",
    "print(\"时间:\", time)\n",
    "print(\"标题:\", title)\n",
    "print(\"内容:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41b1e86c-93db-4745-8c21-b767d2974a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://cpc.people.com.cn/n1/2024/1231/c164113-40393373.html\n",
      "时间: 2024-12-31\n",
      "标题: 时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件-中国共产党新闻网\n",
      "内容: 2024年12月31日晚，国家主席习近平发表二〇二五年新年贺词。习近平主席回顾了2024年中国取得的不平凡成就，激励全国人民“梦虽遥，追则能达；愿虽艰，持则可圆。中国式现代化的新征程上，每一个人都是主,时习之丨习近平：中国式现代化的新征程上，每一个人都是主角--独家稿件\n",
      "--------------------------------------------------\n",
      "URL: http://cpc.people.com.cn/n1/2024/1228/c164113-40391347.html\n",
      "时间: 2024-12-28\n",
      "标题: 时习之丨聚焦2024年中央政治局会议：科学决策 锚定发展航向--独家稿件-中国共产党新闻网\n",
      "内容: ,时习之丨聚焦2024年中央政治局会议：科学决策 锚定发展航向--独家稿件\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_info_from_url(url):\n",
    "    try:\n",
    "        # 发送请求获取页面内容\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "         # 手动指定编码\n",
    "        response.encoding = response.apparent_encoding\n",
    "        \n",
    "        # 解析 HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 提取时间\n",
    "        time_tag = soup.find('meta', {'name': 'publishdate'})\n",
    "        time = time_tag['content'] if time_tag else None\n",
    "\n",
    "        # 提取标题\n",
    "        title_tag = soup.find('title')\n",
    "        title = title_tag.text if title_tag else None\n",
    "\n",
    "        # 提取内容\n",
    "        content_tag = soup.find('meta', {'name': 'description'})\n",
    "        content = content_tag['content'] if content_tag else None\n",
    "\n",
    "        return time, title, content\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"请求 URL {url} 时出错: {e}\")\n",
    "        return None, None, None\n",
    "    except (KeyError, AttributeError) as e:\n",
    "        print(f\"解析 URL {url} 的 HTML 时出错: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def view_multiple_urls(urls):\n",
    "    for url in urls:\n",
    "        time, title, content = extract_info_from_url(url)\n",
    "        print(f\"URL: {url}\")\n",
    "        print(\"时间:\", time)\n",
    "        print(\"标题:\", title)\n",
    "        print(\"内容:\", content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例 URL 列表，你可以替换为实际的 URL\n",
    "    urls = [\n",
    "        \"http://cpc.people.com.cn/n1/2024/1231/c164113-40393373.html\",\n",
    "        \"http://cpc.people.com.cn/n1/2024/1228/c164113-40391347.html\"\n",
    "    ]\n",
    "    view_multiple_urls(urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a2962-a10e-4fa2-85a9-c29d83e26af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
