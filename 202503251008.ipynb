{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cabcb5f-bcd0-4b11-8196-bfd435d834a5",
   "metadata": {},
   "source": [
    "## 第十部分:Python爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a5b73-927d-4739-8e8f-04e8e4b75374",
   "metadata": {},
   "source": [
    "**26.XPath**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fcba8-b1f6-4c9b-8674-8968de4bde1a",
   "metadata": {},
   "source": [
    "**26.1本章工作任务**\n",
    "采用 XPath 语言编写程序,对人民网新闻详情页的信息（新闻的标题、时间、来源和内容）进行抓取。①算法的输入是需要抓取的新闻页面的 URL 地址。②算法模型需要求解的是目标信息所在网页文件中的特征标签和属性名称。③、算法的结果是 URL 的网页中的所有新闻的标题、时间、来源和内容，以列表的形式呈现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dac83d-4c82-4f2b-8423-e05506dfcd97",
   "metadata": {},
   "source": [
    "基于 XPath 的人民网新闻信息抓取方案\n",
    "一、工作任务说明\n",
    "任务目标\n",
    "通过 XPath 语言实现对人民网新闻详情页的信息抓取，提取以下核心内容：\n",
    "新闻标题\n",
    "发布时间\n",
    "新闻来源\n",
    "新闻正文内容\n",
    "输入与输出\n",
    "类型\t描述\t示例格式\n",
    "输入\t新闻详情页 URL\thttps://www.people.com.cn/xxx\n",
    "输出\t结构化新闻信息列表\tJSON/List（包含 4 个字段的字典）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efff55-4638-4615-be7b-e238cd30b205",
   "metadata": {},
   "source": [
    "**26.2本章技能目标**\n",
    "- 掌握爬虫原理\n",
    "- 掌握 Xpath (XML 路径语言)方法原理\n",
    "- 使用 request 包实现对网页 html 代码的获取\n",
    "- 使用 XPath 方法实现对 html 中内容的抓取\n",
    "- 使用 pandas库实现对抓取结果的文件储存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca99bd6-0d26-46ac-8948-d2e586ec51c1",
   "metadata": {},
   "source": [
    "**26.3本章简介**\n",
    "- 数据抓取是指一种将非结构化的数据网站、微博等网页中的网格、文字、图片等按照一定规则来获取，并将获取的数据以结构化的数据表格等的形式进行保存和应用的技术。\n",
    "- XPath是指一种用来确定 x m l 的文档，网页文档中某个标签位置，如标题在网页文档中所处标签的属性，你的查询语言。第五，若网页的源代码为标签开始符号，属性名等于属性值，标签内容、标签结束符号格式可以通过编写代码对象点方法，标签开始符号逗号，属性名等于属性值。定位内容标签位置，通过后续方法获取标签内容。\n",
    "- Xpath以解决的科学问题是对于网页内容，如果内容与标签存在如下关系，标签开始符号属性名等于属性值标签内容标签结束符号，则根据标签符号将标签内容进行提取。\n",
    "- XPath可以解决的实际应用问题是，我们想获得人民日报网站上的新闻信息，包括新闻标题时间来源和内容信息，则可以使用 Xpath 语言来进行逐一定位并获取，并将结果放入列表 list 中并保存文件。\n",
    "- 本章的重点是 x pass 中各种方法的理解和使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a01385-4da5-4b70-b968-a517b9a84fe3",
   "metadata": {},
   "source": [
    "**26.4理论讲解部分**\n",
    "- 26.4.1任务描述\n",
    "- 需要实现的功能描述如下\n",
    "- (1).输入待抓取的页面地址本章采用人民日报网新闻详情页，网址如图所示。\n",
    "- (2).利用 xPath获取目标信息\n",
    "- (3).得到抓取的结果，通过遍历每一条新闻，将获取到的信息存入名为 news 列表中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28873429-4fdf-45e4-a66f-dbf33afb1a63",
   "metadata": {},
   "source": [
    "- 理解XPath的要点如下\n",
    "- (1).方法的输入是需要抓取的新闻页面的 URL 地址。\n",
    "- (2).方法的求解是目标信息所在网页文件中的特征标签和属性名称\n",
    "- (3)方法输出是 URL 网页中所有的新闻标题，时间、来源和内容以列表形式呈现。\n",
    "- (4).方法的核心思想是找到所需信息前后的特征标签，根据特征标签获取，希望抓取内容。\n",
    "- (5).方法的注意事项是 ① 面对反爬虫机制需要设置 ID，网页的 ID 是网页向服务器请求的和响应的核心，它承载了客户端浏览器请求页面服务器的相关的信息，模拟浏览器访问网页。②获取信息有多余字符(如空格)时,需要去掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f5bda5-bc50-4f9d-af5c-977dcbe21ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤1 导入requests包\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from lxml import etree    #Lxml是Python的一个解析库支持HTML和XML的解析，而且解析效率非常高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f45d5cd-5c93-410e-b7b4-481e7deba51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤2 模拟浏览器访问网址，构建浏览器访问网页时的 Headers 标签信息，跳过反爬虫机制以获取信息，可以避免反爬虫机制造成信息获取失败。\n",
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}    #构造请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cf61a-3aaf-4491-beaa-e6d351243cdd",
   "metadata": {},
   "source": [
    "- puzzle1:请求头构造的方法是啥呀，这东西是在哪里看的啊？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb98e13f-34ab-468e-95be-b75167e89952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤3 获取指定URL的页面内容,并根据抓取模板,进行待抓取信息的提取\n",
    "\n",
    "def parse_detail_page(url):\n",
    "    news = {}  #字典news用于存放获取到的新闻信息\n",
    "    response = requests.get(url, headers = HEADERS)    #请求url数据并返回给response对象\n",
    "    text = response.content.decode(\"gb18030\")\n",
    "    #获取response中的信息数据(HTML文本),并进行解码。因为编码方式的不同,获取内容是乱码，需要进行编码转换\n",
    "    html = etree.HTML(text)    #构建一个XPath解析对象html,用来解析字符串格式的text\n",
    "\n",
    "    #获取新闻标题\n",
    "    div_title = html.xpath(\"//div[@class='clearfix w1000_320 text_title']\")[0]\n",
    "    #获取标题所在div。 （此处。。\n",
    "    title = div_title.xpath(\"./h1/text()\")\n",
    "    # 在当前div标签下的h1标签下获取新闻标题\n",
    "    #.表示选取当前节点,\n",
    "    news['title'] = (title[0]).replace('\\xa0','')\n",
    "    #去掉title中的多余字符\"\\xa0\"(空格),将新闻标题存入字典\n",
    "\n",
    "    #获取时间和来源\n",
    "    div_whenandwhere = html.xpath(\"//div[@class='box01']/div[@class='f1']\")[0]\n",
    "    #获取时间和来源所在的div,@表示选取的目标标签的对应属性\n",
    "    title = div_title.xpath(\"./h1/text()\")\n",
    "    when = div_whenandwhere.xpath(\"./text()\")[0].replace(\"\\xa0\",\" \")\n",
    "    #获取新闻发布时间并去掉多余字符\n",
    "    where = div_whenandwhere.xpath(\"./a[@href]/text()\")[0]\n",
    "    #获取新闻来源\n",
    "    news['whenandwhere'] = when + where    #将新闻事件和来源存入news\n",
    "\n",
    "    #获取新闻内容\n",
    "    ps = html.xpath(\"//div[@class='box_con']/p\") #获取所有p标签\n",
    "    contents = []    #列表contents用于存放每个p标签内的新闻内容\n",
    "    for p in ps: #遍历获取到的p标签进行处理\n",
    "        content = p.xpath(\"./text()\")    #获取p标签的文本内容\n",
    "        contents.append((content[0].replace(\"\\n\\t\",' ')).replace(\"\\u3000\\u3000\",''))\n",
    "        #去掉content中的多余字符并将其追加至contents列表\n",
    "    news['content'] = contents    #将新闻内容存入字典news\n",
    "    return news    #返回news字典    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34aba84-de87-44ac-a1ff-e84afee7de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤4 获取指定页面的内容,存入.csv或者.xls文件。定义函数名为spider(),获取指定URL的新闻详情页内容。\n",
    "def spider(urls):\n",
    "    news = []    #用于存放获取到的新闻详情页信息\n",
    "    for url in urls:    #遍历详情页面的url列表,对每个详情页分别进行解析\n",
    "        new = parse_detail_page(url)    \n",
    "        #解析获取到的详情页，返回给用于存放新闻信息的字典new\n",
    "        news.append(new)    #将字典new追加到列表news中\n",
    "    print(news)    #打印获取到的新闻详情页信息\n",
    "\n",
    "    df = pd.DataFrame(news, columns = ['title', 'whenandwhere', 'content'])\n",
    "    #使用字典news构建DataFrame对象df，指定列名为'title'、'whenandwhere'、'content'\n",
    "    df.to_csv('news.csv')    #将df对象写入csv文件\n",
    "    df.to_excel('news.xls')    #将df对象写入xls文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177df47-d586-444c-8c22-772f18a3a151",
   "metadata": {},
   "source": [
    "- puzzle3:一堆字典列表[ { } ]打印出来的是啥东西？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fd3492-4c69-4978-92d0-397570bd1e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025481.html\u001b[39m\u001b[38;5;124m'\u001b[39m ,\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025480.html\u001b[39m\u001b[38;5;124m'\u001b[39m ]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#指定待抓取的新闻详情页url\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mspider\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m#调用spider(),获取并打印指定url的新闻详情页内容\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mspider\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      3\u001b[0m news \u001b[38;5;241m=\u001b[39m []    \u001b[38;5;66;03m#用于存放获取到的新闻详情页信息\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:    \u001b[38;5;66;03m#遍历详情页面的url列表,对每个详情页分别进行解析\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43mparse_detail_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#解析获取到的详情页，返回给用于存放新闻信息的字典new\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     news\u001b[38;5;241m.\u001b[39mappend(new)    \u001b[38;5;66;03m#将字典new追加到列表news中\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mparse_detail_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     16\u001b[0m news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (title[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#去掉title中的多余字符\"\\xa0\"(空格),将新闻标题存入字典\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#获取时间和来源\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m div_whenandwhere \u001b[38;5;241m=\u001b[39m \u001b[43mhtml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//div[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbox01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]/div[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#获取时间和来源所在的div,@表示选取的目标标签的对应属性\u001b[39;00m\n\u001b[0;32m     22\u001b[0m title \u001b[38;5;241m=\u001b[39m div_title\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./h1/text()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 步骤5：指定URL的值，调用spider()进行页面内容的获取\n",
    "urls = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html' ,\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html' ]\n",
    "#指定待抓取的新闻详情页url\n",
    "spider(urls)    #调用spider(),获取并打印指定url的新闻详情页内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3934135-5591-4a98-8715-c8d9c42b0cbd",
   "metadata": {},
   "source": [
    "调用DeepSeek---解决网页结构改变\n",
    "综上，现在需要做的是：\n",
    "\n",
    "1. 检查网页结构，确认时间和来源的XPath路径。\n",
    "\n",
    "2. 修改代码中的XPath表达式，或者处理可能的空列表情况。\n",
    "\n",
    "例如，假设在目标网页中，时间和来源的div位于：\n",
    "\n",
    "//div[@class='col-1-1 fl']\n",
    "\n",
    "那么，正确的XPath应该是：\n",
    "\n",
    "div_whenandwhere = html.xpath(\"//div[@class='col-1-1 fl']\")[0]\n",
    "\n",
    "或者，可能需要更精确的路径。\n",
    "\n",
    "或者，可能时间和来源所在的元素结构已经改变，所以原来的XPath无法找到元素。\n",
    "\n",
    "例如，在某个例子中，时间和来源的XPath可能是：\n",
    "\n",
    "//div[@class='channel']/span/text()\n",
    "\n",
    "此时，原来的XPath表达式无法正确获取。\n",
    "\n",
    "因此，修改代码中的XPath表达式以匹配当前页面结构即可解决问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed0b166-af4f-4fb7-84c5-ed8c6b1204b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}\n",
    "\n",
    "def parse_detail_page(url):\n",
    "    news = {}\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    text = response.content.decode(\"gb18030\")\n",
    "    html = etree.HTML(text)\n",
    "\n",
    "    # 获取新闻标题\n",
    "    title_element = html.xpath(\"//h1[@class='title']/text()\")\n",
    "    if title_element:\n",
    "        news['title'] = title_element[0].replace('\\xa0', '')\n",
    "    else:\n",
    "        news['title'] = ''\n",
    "\n",
    "    # 获取时间和来源\n",
    "    # 假设时间和来源位于某个特定的div或span中，调整XPath\n",
    "    # 示例：假设时间为class='date'，来源为class='source'\n",
    "    time_element = html.xpath(\"//div[@class='article-info']/span[1]/text()\")\n",
    "    source_element = html.xpath(\"//div[@class='article-info']/span[2]/a/text()\")\n",
    "    \n",
    "    when = time_element[0].strip() if time_element else ''\n",
    "    where = source_element[0].strip() if source_element else ''\n",
    "    news['whenandwhere'] = f\"{when} {where}\"\n",
    "\n",
    "    # 获取新闻内容\n",
    "    content_paragraphs = html.xpath(\"//div[@class='article-content']//p/text()\")\n",
    "    contents = [p.strip().replace('\\u3000', ' ') for p in content_paragraphs if p.strip()]\n",
    "    news['content'] = ' '.join(contents)\n",
    "\n",
    "    return news\n",
    "\n",
    "def spider(urls):\n",
    "    news_list = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            news = parse_detail_page(url)\n",
    "            news_list.append(news)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(news_list, columns=['title', 'whenandwhere', 'content'])\n",
    "    df.to_csv('news.csv', index=False)\n",
    "    df.to_excel('news.xlsx', index=False)\n",
    "\n",
    "urls = [\n",
    "    'http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html',\n",
    "    'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html'\n",
    "]\n",
    "spider(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d433fb-c348-439e-95e1-bd3335b625f4",
   "metadata": {},
   "source": [
    "- 调用豆包\n",
    "你遇到的 IndexError: list index out of range 错误表明，在代码里尝试访问一个空列表的元素。具体而言，错误出现在 div_whenandwhere = html.xpath(\"//div[@class='box01']/div[@class='f1']\")[0] 这一行，这意味着 html.xpath(\"//div[@class='box01']/div[@class='f1']\") 返回的列表为空，所以无法访问其索引为 0 的元素。\n",
    "这种情况通常是由于网页结构发生了变化，导致 XPath 表达式无法找到对应的元素。以下是对代码的修改建议，添加了错误处理，避免因找不到元素而引发异常：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8a3951-7dcf-436d-91f5-122cfd94d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '韩正会见美国前财政部长保尔森', 'whenandwhere': None, 'content': [' 新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光阁会见美国前财政部长保尔森。', ' 韩正表示，中美建交40年来，两国关系经历各种考验，取得了历史性发展。中美在维护世界和平稳定、促进全球发展繁荣方面拥有广泛利益、肩负着重要责任。要在两国元首重要共识的指引下，推动中美经贸合作和两国关系健康稳定向前发展。加强生态环境保护是中国实现高质量发展的必然要求，中方愿意开展各种形式的国际合作，愿意与保尔森基金会共同努力，在生态环境保护方面进一步拓宽合作领域。', ' 保尔森表示，美中关系是世界上最重要的双边关系之一。保尔森基金会愿意积极推动美中贸易投资、环境保护、清洁能源、绿色金融等领域的合作。']}, {'title': '习近平与独龙族的故事', 'whenandwhere': None, 'content': [' 新华网 金佳绪', ' ', ' 4月10日，习近平给云南省贡山县独龙江乡群众回信，祝贺独龙族实现整族脱贫。', ' 总书记说，得知这个消息，我很高兴，向你们表示衷心的祝贺！', ' ', ' 2018年，独龙江乡6个行政村整体脱贫，独龙族实现整族脱贫，当地群众委托乡党委给习近平总书记写信，汇报独龙族实现整族脱贫的喜讯。', ' 2019年4月10日，习近平给乡亲们回信，祝贺独龙族实现整族脱贫。', ' 在信里，习近平说：', ' “让各族群众都过上好日子，是我一直以来的心愿，也是我们共同奋斗的目标。新中国成立后，独龙族告别了刀耕火种的原始生活。进入新时代，独龙族摆脱了长期存在的贫困状况。这生动说明，有党的坚强领导，有广大人民群众的团结奋斗，人民追求幸福生活的梦想一定能够实现。”', ' “脱贫只是第一步，更好的日子还在后头。”习近平勉励乡亲们再接再厉、奋发图强，同心协力建设好家乡、守护好边疆，努力创造独龙族更加美好的明天。', ' ', ' 独龙族是我国28个人口较少民族之一，也是新中国成立初期一个从原始社会末期直接过渡到社会主义社会的少数民族，主要聚居在云南省贡山县独龙江乡。当地地处深山峡谷，自然条件恶劣，一直是云南乃至全国最为贫穷的地区之一。', ' 摆脱贫困，过上美好生活，这是独龙族同胞一直以来的期盼。如今，这个愿望变成了现实。', ' 2018年年底，作为人口较少的“直过民族”，独龙族从整体贫困实现了整族脱贫，贫困发生率下降到了2.63%，独龙江乡1086户群众全部住进了新房，所有自然村都通了硬化路，4G网络、广播电视信号覆盖到全乡，种草果、采蜂蜜、养独龙牛，乡亲们的收入增加了，孩子们享受着14年免费教育，群众看病有了保障……', ' ', ' 很多人对这个生活在偏远地区人数较少的少数民族有些陌生，知之甚少。但习近平却表示，“我们并不陌生，因为有书信往来。”', ' 2014年元旦前夕，贡山县干部群众致信习近平总书记，汇报了当地经济社会发展和人民生活改善的情况，报告了多年期盼的高黎贡山独龙江公路隧道即将贯通的消息，习近平接到信后立即给他们回信：“向独龙族的乡亲们表示祝贺！”希望独龙族群众“加快脱贫致富步伐，早日实现与全国其他兄弟民族一道过上小康生活的美好梦想”。', ' 总书记对独龙族同胞的牵挂，远不止书信。', ' 2015年1月，习近平在云南考察。他仍关注着高黎贡山隧道建设，关注着独龙族干部群众生活发生的变化。带着对贡山县干部群众尤其是独龙族乡亲们的惦念，习近平在这次紧张的行程中特地抽出时间，把当初写信的5位干部群众和2位独龙族妇女，专程接到昆明来见面。', ' “建一套新房多少钱？”“原来出山要多长时间？”……此次见面，习近平对乡亲们的生活情况细问详察，共同分享沧桑巨变带来的喜悦，对干部群众寄语频频。', ' “我今天特别高兴，能够在这里同贡山独龙族怒族自治县的代表们见面。”习近平说，独龙族这个名字是周总理起的，虽然只有6900多人，人口不多，也是中华民族大家庭平等的一员，在中华人民共和国、中华民族大家庭之中骄傲地、有尊严地生活着，在中国共产党领导下，同各民族人民一起努力工作，为全面建成小康社会的目标奋斗。', ' 总书记指出，独龙族和其他一些少数民族的沧桑巨变，证明了中国特色社会主义制度的优越性。前面的任务还很艰巨，我们要继续发挥我国制度的优越性，继续把工作做好、事情办好。', ' “全面建成小康社会，一个民族都不能少”，是全国人民的心愿，更是以习近平同志为核心的党中央的坚定决心。当又一个少数民族整体脱贫的好消息传来，总书记怎能不由衷高兴！', ' ', ' 点击进入专题']}]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No engine for filetype: 'xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOptionError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1111\u001b[0m, in \u001b[0;36mExcelWriter.__new__\u001b[1;34m(cls, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1111\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.excel.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.writer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\_config\\config.py:261\u001b[0m, in \u001b[0;36mCallableDynamicDoc.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\_config\\config.py:135\u001b[0m, in \u001b[0;36m_get_option\u001b[1;34m(pat, silent)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_option\u001b[39m(pat: \u001b[38;5;28mstr\u001b[39m, silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 135\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43m_get_single_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# walk the nested dict\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\_config\\config.py:121\u001b[0m, in \u001b[0;36m_get_single_key\u001b[1;34m(pat, silent)\u001b[0m\n\u001b[0;32m    120\u001b[0m         _warn_if_deprecated(pat)\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OptionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such keys(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(pat)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mOptionError\u001b[0m: \"No such keys(s): 'io.excel.xls.writer'\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 指定待抓取的新闻详情页url\u001b[39;00m\n\u001b[0;32m     84\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025481.html\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://politics.people.com.cn/n1/2019/0411/c1024-31025480.html\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 86\u001b[0m \u001b[43mspider\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 调用spider(),获取并打印指定url的新闻详情页内容\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 81\u001b[0m, in \u001b[0;36mspider\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 使用字典news构建DataFrame对象df，指定列名为'title'、'whenandwhere'、'content'\u001b[39;00m\n\u001b[0;32m     80\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)    \u001b[38;5;66;03m# 将df对象写入csv文件\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews.xls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\core\\generic.py:2252\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2241\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2242\u001b[0m     df,\n\u001b[0;32m   2243\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2250\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2251\u001b[0m )\n\u001b[1;32m-> 2252\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\formats\\excel.py:934\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    930\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[0;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1115\u001b[0m, in \u001b[0;36mExcelWriter.__new__\u001b[1;34m(cls, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1113\u001b[0m             engine \u001b[38;5;241m=\u001b[39m get_default_engine(ext, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo engine for filetype: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: No engine for filetype: 'xls'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "# 构造请求头\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}\n",
    "\n",
    "def parse_detail_page(url):\n",
    "    news = {}  # 字典news用于存放获取到的新闻信息\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)    # 请求url数据并返回给response对象\n",
    "        text = response.content.decode(\"gb18030\")\n",
    "        # 获取response中的信息数据(HTML文本),并进行解码。因为编码方式的不同,获取内容是乱码，需要进行编码转换\n",
    "        html = etree.HTML(text)    # 构建一个XPath解析对象html,用来解析字符串格式的text\n",
    "\n",
    "        # 获取新闻标题\n",
    "        div_title_list = html.xpath(\"//div[@class='clearfix w1000_320 text_title']\")\n",
    "        if div_title_list:\n",
    "            div_title = div_title_list[0]\n",
    "            title = div_title.xpath(\"./h1/text()\")\n",
    "            if title:\n",
    "                news['title'] = (title[0]).replace('\\xa0', '')\n",
    "            else:\n",
    "                news['title'] = None\n",
    "        else:\n",
    "            news['title'] = None\n",
    "\n",
    "        # 获取时间和来源\n",
    "        div_whenandwhere_list = html.xpath(\"//div[@class='box01']/div[@class='f1']\")\n",
    "        if div_whenandwhere_list:\n",
    "            div_whenandwhere = div_whenandwhere_list[0]\n",
    "            when_list = div_whenandwhere.xpath(\"./text()\")\n",
    "            if when_list:\n",
    "                when = when_list[0].replace(\"\\xa0\", \" \")\n",
    "            else:\n",
    "                when = None\n",
    "            where_list = div_whenandwhere.xpath(\"./a[@href]/text()\")\n",
    "            if where_list:\n",
    "                where = where_list[0]\n",
    "            else:\n",
    "                where = None\n",
    "            if when and where:\n",
    "                news['whenandwhere'] = when + where\n",
    "            else:\n",
    "                news['whenandwhere'] = None\n",
    "        else:\n",
    "            news['whenandwhere'] = None\n",
    "\n",
    "        # 获取新闻内容\n",
    "        ps = html.xpath(\"//div[@class='box_con']/p\")  # 获取所有p标签\n",
    "        contents = []    # 列表contents用于存放每个p标签内的新闻内容\n",
    "        for p in ps:  # 遍历获取到的p标签进行处理\n",
    "            content = p.xpath(\"./text()\")    # 获取p标签的文本内容\n",
    "            if content:\n",
    "                contents.append((content[0].replace(\"\\n\\t\", ' ')).replace(\"\\u3000\\u3000\", ''))\n",
    "        news['content'] = contents if contents else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"解析页面 {url} 时出错: {e}\")\n",
    "        news = {\n",
    "            'title': None,\n",
    "            'whenandwhere': None,\n",
    "            'content': None\n",
    "        }\n",
    "\n",
    "    return news\n",
    "\n",
    "def spider(urls):\n",
    "    news = []    # 用于存放获取到的新闻详情页信息\n",
    "    for url in urls:    # 遍历详情页面的url列表,对每个详情页分别进行解析\n",
    "        new = parse_detail_page(url)\n",
    "        news.append(new)    # 将字典new追加到列表news中\n",
    "    print(news)    # 打印获取到的新闻详情页信息\n",
    "\n",
    "    df = pd.DataFrame(news, columns=['title', 'whenandwhere', 'content'])\n",
    "    # 使用字典news构建DataFrame对象df，指定列名为'title'、'whenandwhere'、'content'\n",
    "    df.to_csv('news.csv')    # 将df对象写入csv文件\n",
    "    df.to_excel('news.xls')    # 将df对象写入xls文件\n",
    "\n",
    "# 指定待抓取的新闻详情页url\n",
    "urls = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html',\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html']\n",
    "spider(urls)    # 调用spider(),获取并打印指定url的新闻详情页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1141ef6f-072f-41e1-92a8-b25184ddefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '韩正会见美国前财政部长保尔森', 'whenandwhere': None, 'content': [' 新华社北京4月11日电（记者潘洁）中共中央政治局常委、国务院副总理韩正11日在中南海紫光阁会见美国前财政部长保尔森。', ' 韩正表示，中美建交40年来，两国关系经历各种考验，取得了历史性发展。中美在维护世界和平稳定、促进全球发展繁荣方面拥有广泛利益、肩负着重要责任。要在两国元首重要共识的指引下，推动中美经贸合作和两国关系健康稳定向前发展。加强生态环境保护是中国实现高质量发展的必然要求，中方愿意开展各种形式的国际合作，愿意与保尔森基金会共同努力，在生态环境保护方面进一步拓宽合作领域。', ' 保尔森表示，美中关系是世界上最重要的双边关系之一。保尔森基金会愿意积极推动美中贸易投资、环境保护、清洁能源、绿色金融等领域的合作。']}, {'title': '习近平与独龙族的故事', 'whenandwhere': None, 'content': [' 新华网 金佳绪', ' ', ' 4月10日，习近平给云南省贡山县独龙江乡群众回信，祝贺独龙族实现整族脱贫。', ' 总书记说，得知这个消息，我很高兴，向你们表示衷心的祝贺！', ' ', ' 2018年，独龙江乡6个行政村整体脱贫，独龙族实现整族脱贫，当地群众委托乡党委给习近平总书记写信，汇报独龙族实现整族脱贫的喜讯。', ' 2019年4月10日，习近平给乡亲们回信，祝贺独龙族实现整族脱贫。', ' 在信里，习近平说：', ' “让各族群众都过上好日子，是我一直以来的心愿，也是我们共同奋斗的目标。新中国成立后，独龙族告别了刀耕火种的原始生活。进入新时代，独龙族摆脱了长期存在的贫困状况。这生动说明，有党的坚强领导，有广大人民群众的团结奋斗，人民追求幸福生活的梦想一定能够实现。”', ' “脱贫只是第一步，更好的日子还在后头。”习近平勉励乡亲们再接再厉、奋发图强，同心协力建设好家乡、守护好边疆，努力创造独龙族更加美好的明天。', ' ', ' 独龙族是我国28个人口较少民族之一，也是新中国成立初期一个从原始社会末期直接过渡到社会主义社会的少数民族，主要聚居在云南省贡山县独龙江乡。当地地处深山峡谷，自然条件恶劣，一直是云南乃至全国最为贫穷的地区之一。', ' 摆脱贫困，过上美好生活，这是独龙族同胞一直以来的期盼。如今，这个愿望变成了现实。', ' 2018年年底，作为人口较少的“直过民族”，独龙族从整体贫困实现了整族脱贫，贫困发生率下降到了2.63%，独龙江乡1086户群众全部住进了新房，所有自然村都通了硬化路，4G网络、广播电视信号覆盖到全乡，种草果、采蜂蜜、养独龙牛，乡亲们的收入增加了，孩子们享受着14年免费教育，群众看病有了保障……', ' ', ' 很多人对这个生活在偏远地区人数较少的少数民族有些陌生，知之甚少。但习近平却表示，“我们并不陌生，因为有书信往来。”', ' 2014年元旦前夕，贡山县干部群众致信习近平总书记，汇报了当地经济社会发展和人民生活改善的情况，报告了多年期盼的高黎贡山独龙江公路隧道即将贯通的消息，习近平接到信后立即给他们回信：“向独龙族的乡亲们表示祝贺！”希望独龙族群众“加快脱贫致富步伐，早日实现与全国其他兄弟民族一道过上小康生活的美好梦想”。', ' 总书记对独龙族同胞的牵挂，远不止书信。', ' 2015年1月，习近平在云南考察。他仍关注着高黎贡山隧道建设，关注着独龙族干部群众生活发生的变化。带着对贡山县干部群众尤其是独龙族乡亲们的惦念，习近平在这次紧张的行程中特地抽出时间，把当初写信的5位干部群众和2位独龙族妇女，专程接到昆明来见面。', ' “建一套新房多少钱？”“原来出山要多长时间？”……此次见面，习近平对乡亲们的生活情况细问详察，共同分享沧桑巨变带来的喜悦，对干部群众寄语频频。', ' “我今天特别高兴，能够在这里同贡山独龙族怒族自治县的代表们见面。”习近平说，独龙族这个名字是周总理起的，虽然只有6900多人，人口不多，也是中华民族大家庭平等的一员，在中华人民共和国、中华民族大家庭之中骄傲地、有尊严地生活着，在中国共产党领导下，同各民族人民一起努力工作，为全面建成小康社会的目标奋斗。', ' 总书记指出，独龙族和其他一些少数民族的沧桑巨变，证明了中国特色社会主义制度的优越性。前面的任务还很艰巨，我们要继续发挥我国制度的优越性，继续把工作做好、事情办好。', ' “全面建成小康社会，一个民族都不能少”，是全国人民的心愿，更是以习近平同志为核心的党中央的坚定决心。当又一个少数民族整体脱贫的好消息传来，总书记怎能不由衷高兴！', ' ', ' 点击进入专题']}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "# 构造请求头\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}\n",
    "\n",
    "def parse_detail_page(url):\n",
    "    news = {}  # 字典news用于存放获取到的新闻信息\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)    # 请求url数据并返回给response对象\n",
    "        text = response.content.decode(\"gb18030\")\n",
    "        # 获取response中的信息数据(HTML文本),并进行解码。因为编码方式的不同,获取内容是乱码，需要进行编码转换\n",
    "        html = etree.HTML(text)    # 构建一个XPath解析对象html,用来解析字符串格式的text\n",
    "\n",
    "        # 获取新闻标题\n",
    "        div_title_list = html.xpath(\"//div[@class='clearfix w1000_320 text_title']\")\n",
    "        if div_title_list:\n",
    "            div_title = div_title_list[0]\n",
    "            title = div_title.xpath(\"./h1/text()\")\n",
    "            if title:\n",
    "                news['title'] = (title[0]).replace('\\xa0', '')\n",
    "            else:\n",
    "                news['title'] = None\n",
    "        else:\n",
    "            news['title'] = None\n",
    "\n",
    "        # 获取时间和来源\n",
    "        div_whenandwhere_list = html.xpath(\"//div[@class='box01']/div[@class='f1']\")\n",
    "        if div_whenandwhere_list:\n",
    "            div_whenandwhere = div_whenandwhere_list[0]\n",
    "            when_list = div_whenandwhere.xpath(\"./text()\")\n",
    "            if when_list:\n",
    "                when = when_list[0].replace(\"\\xa0\", \" \")\n",
    "            else:\n",
    "                when = None\n",
    "            where_list = div_whenandwhere.xpath(\"./a[@href]/text()\")\n",
    "            if where_list:\n",
    "                where = where_list[0]\n",
    "            else:\n",
    "                where = None\n",
    "            if when and where:\n",
    "                news['whenandwhere'] = when + where\n",
    "            else:\n",
    "                news['whenandwhere'] = None\n",
    "        else:\n",
    "            news['whenandwhere'] = None\n",
    "\n",
    "        # 获取新闻内容\n",
    "        ps = html.xpath(\"//div[@class='box_con']/p\")  # 获取所有p标签\n",
    "        contents = []    # 列表contents用于存放每个p标签内的新闻内容\n",
    "        for p in ps:  # 遍历获取到的p标签进行处理\n",
    "            content = p.xpath(\"./text()\")    # 获取p标签的文本内容\n",
    "            if content:\n",
    "                contents.append((content[0].replace(\"\\n\\t\", ' ')).replace(\"\\u3000\\u3000\", ''))\n",
    "        news['content'] = contents if contents else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"解析页面 {url} 时出错: {e}\")\n",
    "        news = {\n",
    "            'title': None,\n",
    "            'whenandwhere': None,\n",
    "            'content': None\n",
    "        }\n",
    "\n",
    "    return news\n",
    "\n",
    "def spider(urls):\n",
    "    news = []    # 用于存放获取到的新闻详情页信息\n",
    "    for url in urls:    # 遍历详情页面的url列表,对每个详情页分别进行解析\n",
    "        new = parse_detail_page(url)\n",
    "        news.append(new)    # 将字典new追加到列表news中\n",
    "    print(news)    # 打印获取到的新闻详情页信息\n",
    "\n",
    "    df = pd.DataFrame(news, columns=['title', 'whenandwhere', 'content'])\n",
    "    # 使用字典news构建DataFrame对象df，指定列名为'title'、'whenandwhere'、'content'\n",
    "    df.to_csv('news.csv')    # 将df对象写入csv文件\n",
    "    #df.to_excel('news.xls',engine='xlwt')    # 将df对象写入xls文件\n",
    "\n",
    "# 指定待抓取的新闻详情页url\n",
    "urls = ['http://politics.people.com.cn/n1/2019/0411/c1024-31025481.html',\n",
    "        'http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html']\n",
    "spider(urls)    # 调用spider(),获取并打印指定url的新闻详情页内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce111305-bdfe-44e3-a51b-fa560d1686d4",
   "metadata": {},
   "source": [
    "- 我得到的结果和书上的不一样，犹豫了以下，纠结了半天，实现有些瑕疵。不过转过头一看，原来是自己代码，抓取的网页whenandwhere为None，那就算解决了啊。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0e5e0-de0d-4eff-8f99-66adcf79d618",
   "metadata": {},
   "source": [
    "**本章作业:抓取新浪新闻详情页最新的一条新闻内容，并将结果导入.csv文件和.xlsx文件中。**\n",
    "- 请课后后完成2025-03-25-14：25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96b47d-a947-455e-aae5-2b75ea4c8cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca5436-38a0-4897-aae6-f30f062d75ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ed2c5-110d-433d-8e91-fc1584ba3646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fd5c2-c1a4-4211-aef2-f3528d150e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0208e2-17e4-4bdf-a259-97070e8b69c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9b6e07-c8da-462d-8591-fca313f7a27e",
   "metadata": {},
   "source": [
    "- puzzle2:抓取到详情页面后，html格式的信息说了啥，想要保存为csv文件，格式转化的方法是啥？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21329098-8bb2-4c83-83ee-938cba598ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea6f477-443d-47d7-9a77-beea1bb56fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 results.csv\n"
     ]
    }
   ],
   "source": [
    "#这是大语言模型给出的一段程序修改了url后可以保存为一个csv文件文件内容\"未知\"\n",
    "\n",
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    发送HTTP请求，获取网页的HTML代码\n",
    "    :param url: 目标网页的URL\n",
    "    :return: 网页的HTML代码\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"请求出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_html(html):\n",
    "    \"\"\"\n",
    "    使用XPath解析HTML代码，提取所需信息\n",
    "    :param html: 网页的HTML代码\n",
    "    :return: 提取的信息列表\n",
    "    \"\"\"\n",
    "    if html:\n",
    "        tree = etree.HTML(html)\n",
    "        # 示例：提取所有a标签的文本和链接\n",
    "        links = tree.xpath('//a')\n",
    "        results = []\n",
    "        for link in links:\n",
    "            text = link.text\n",
    "            href = link.get('href')\n",
    "            results.append({'text': text, 'href': href})\n",
    "        return results\n",
    "    return []\n",
    "\n",
    "def save_to_file(results, filename):\n",
    "    \"\"\"\n",
    "    使用pandas将抓取结果保存为CSV文件\n",
    "    :param results: 提取的信息列表\n",
    "    :param filename: 保存的文件名\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"数据已保存到 {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'http://opinion.people.com.cn/n1/2025/0325/c1003-40445766.html'  # 替换为实际的URL\n",
    "    html = get_html(url)\n",
    "    results = parse_html(html)\n",
    "    save_to_file(results, 'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b75e7e-1c36-4810-819c-95e2f7ecb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未获取到新闻信息\n"
     ]
    }
   ],
   "source": [
    "#这是测试失败的大预言模型(2)\n",
    "\n",
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "def get_news_info(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "        tree = etree.HTML(html)\n",
    "\n",
    "        # 假设这里是人民日报新闻页面的 XPath 表达式，实际需要根据页面结构调整\n",
    "        titles = tree.xpath('//h3[@class=\"title\"]/text()')\n",
    "        times = tree.xpath('//span[@class=\"time\"]/text()')\n",
    "        sources = tree.xpath('//span[@class=\"source\"]/text()')\n",
    "        contents = tree.xpath('//div[@class=\"content\"]/text()')\n",
    "\n",
    "        news_list = []\n",
    "        for title, time, source, content in zip(titles, times, sources, contents):\n",
    "            news_list.append({\n",
    "                '标题': title.strip(),\n",
    "                '时间': time.strip(),\n",
    "                '来源': source.strip(),\n",
    "                '内容': content.strip()\n",
    "            })\n",
    "        return news_list\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"请求出错: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'http://politics.people.com.cn/n1/2025/0325/c1001-40445729.html'  # 替换为实际的人民日报新闻页面 URL\n",
    "    news_info = get_news_info(url)\n",
    "    if news_info:\n",
    "        df = pd.DataFrame(news_info)\n",
    "        df.to_csv('news_info.csv', index=False)\n",
    "        print(\"新闻信息已保存到 news_info.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻信息\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa0edb9-50a6-4ae0-87d8-b684ea756a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取到的信息：\n",
      "http://www.people.com.cn/\n",
      "http://cpc.people.com.cn/\n",
      "http://finance.people.com.cn/\n",
      "http://society.people.com.cn/\n",
      "http://ent.people.com.cn/\n",
      "http://health.people.com.cn/\n",
      "http://opinion.people.com.cn/\n",
      "http://fangtan.people.com.cn/\n",
      "http://world.people.com.cn/\n",
      "http://military.people.com.cn/\n",
      "http://hm.people.com.cn/\n",
      "http://tw.people.com.cn/\n",
      "http://v.people.cn/\n",
      "http://pic.people.com.cn/\n",
      "http://edu.people.com.cn/\n",
      "http://house.people.com.cn/\n",
      "http://sso.people.com.cn/u/reg?appCode=ENw9NE44\n",
      "http://sso.people.com.cn/u/reg?appCode=ENw9NE44\n",
      "http://sso.people.com.cn/u/findpwd/user\n",
      "http://www.people.com.cn\n",
      "http://www.people.com.cn\n",
      "http://www.people.com.cn/\n",
      "http://politics.people.com.cn/GB/1024/\n",
      "http://www.people.com.cn/GB/138812/index.html\n",
      "http://www.xinhuanet.com/politics/xxjxs/2019-04/11/c_1124355291.htm\n",
      "http://bbs1.people.com.cn/postLink.do?nid=31025480\n",
      "http://www.people.com.cn/GB/123231/365206/index.html\n",
      "http://www.people.com.cn/GB/123231/365208/index.html\n",
      "http://5g.people.cn/rmspdown/\n",
      "http://www.people.cn/rmzy/download.html\n",
      "http://leaders.people.com.cn/GB/178291/407226/index.html\n",
      "http://coo.people.cn/\n",
      "http://bbs1.people.com.cn/postLink.do?nid=31025480\n",
      "/n1/2020/1228/c1001-31981738.html\n",
      "/n1/2020/1222/c1001-31974715.html\n",
      "/n1/2020/1223/c1001-31976569.html\n",
      "/n1/2020/1224/c1001-31978076.html\n",
      "/n1/2020/1222/c1001-31975113.html\n",
      "http://politics.people.com.cn/GB/432731/\n",
      "http://politics.people.com.cn/GB/432731/\n",
      "http://politics.people.com.cn/GB/8198/434128/index.html\n",
      "http://politics.people.com.cn/GB/8198/434128/index.html\n",
      "http://politics.people.com.cn/GB/8198/432907/index.html\n",
      "http://politics.people.com.cn/GB/8198/432907/index.html\n",
      "http://politics.people.com.cn/GB/431485/index.html\n",
      "http://politics.people.com.cn/GB/431485/index.html\n",
      "https://apiapp.people.cn/a/a/m/content_wap_319427.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_319427.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_319432.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_319432.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_318019.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_318019.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_318654.shtml\n",
      "https://apiapp.people.cn/a/a/m/content_wap_318654.shtml\n",
      "http://v.people.cn/n1/2020/1209/c413792-31960779.html\n",
      "http://v.people.cn/n1/2020/1209/c413792-31960783.html\n",
      "http://v.people.cn/n1/2020/1210/c201221-31962188.html\n",
      "/n1/2021/0610/c1024-32127011.html\n",
      "/n1/2021/0610/c1001-32127028.html\n",
      "/n1/2021/0610/c1024-32127015.html\n",
      "/n1/2021/0610/c1001-32127078.html\n",
      "/n1/2021/0610/c1001-32127974.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "# 构造请求头\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "}\n",
    "\n",
    "def get_web_page_content(url):\n",
    "    try:\n",
    "        # 发送请求，携带构造好的请求头\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        # 检查请求是否成功\n",
    "        response.raise_for_status()\n",
    "        # 返回网页的 HTML 内容\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"请求网页时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_info(html_content):\n",
    "    if html_content:\n",
    "        # 解析 HTML 内容\n",
    "        tree = etree.HTML(html_content)\n",
    "        # 这里需要根据实际网页结构修改 XPath 表达式\n",
    "        # 示例：提取所有链接\n",
    "        links = tree.xpath('//a/@href')\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 替换为你要访问的实际网址\n",
    "    target_url = \"http://politics.people.com.cn/n1/2019/0411/c1024-31025480.html\"\n",
    "    page_content = get_web_page_content(target_url)\n",
    "    if page_content:\n",
    "        info = extract_info(page_content)\n",
    "        if info:\n",
    "            print(\"提取到的信息：\")\n",
    "            for item in info:\n",
    "                print(item)\n",
    "        else:\n",
    "            print(\"未提取到有效信息。\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e978e4-9abc-42e4-b49b-bfec8a379f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
